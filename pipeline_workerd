#!/usr/bin/env python3
#
# This file is part of pipelined.
#
# pipelined is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# pipelined is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with pipelined.  If not, see <http://www.gnu.org/licenses/>.

"""Frame pipeline daemon"""

# pylint: disable=too-many-arguments
# pylint: disable=too-many-branches
# pylint: disable=too-many-locals
# pylint: disable=too-many-statements


import argparse
import array
import datetime
import json
import multiprocessing
import os
import queue
import re
import shutil
import socket
import subprocess
import sys
import tempfile
import threading
import traceback
from astropy.coordinates import Angle
from astropy.io import fits
from astropy.table import Table
from astropy.time import Time
import astropy.units as u
from astropy.wcs import WCS
import numpy as np
from paramiko import SSHClient, RSAKey
from PIL import Image, ImageOps
import pyds9
import Pyro4
from scp import SCPClient
import sep
from warwick.observatory.common import daemons, log, IP
from warwick.observatory.common.helpers import pyro_client_matches
from warwick.observatory.pipeline import CommandStatus, Config

# Set automatically when generating RPM package
SOFTWARE_VERSION = 'UNKNOWN'

ANNOTATION_LABEL = 'image; text {} {} #color=green font="helvetica 12 bold" text="{}" select=0'


def window_header_region(header, data, keyword):
    """Returns a cropped view of frame data"""
    if keyword not in header:
        return data, 0, 0

    region = header[keyword]
    r = re.search(r'^\[(\d+):(\d+),(\d+):(\d+)\]$', region).groups()
    x1 = int(r[0]) - 1
    x2 = int(r[1])
    y1 = int(r[2]) - 1
    y2 = int(r[3])
    return data[y1:y2, x1:x2], x1, y1


def rescale_image_data(data, clip_low, clip_high):
    """ Returns a normalised array where clip_low percent of the pixels are 0 and
        clip_high percent of the pixels are 255
    """
    high = np.percentile(data, clip_high)
    low = np.percentile(data, clip_low)
    scale = 255. / (high - low)
    data = np.clip(data, low, high)
    return scale * (data - low)


def create_formatted_card(key, value, comment, format_type):
    float_places = {
        'float1dp': 1,
        'float2dp': 2,
        'float5dp': 5
    }

    try:
        if format_type in float_places:
            return create_float_card(key, value, comment, float_places[format_type])

        if format_type == 'int':
            value = int(value)
        elif format_type == 'bool':
            value = bool(value)
        elif format_type == 'sexagesimalha':
            value = Angle(value * u.deg).to(u.hourangle).to_string(sep=':', precision=2)
        elif format_type == 'sexagesimaldeg':
            value = Angle(value * u.deg).to_string(sep=':', precision=2)

        return fits.Card(key, value, comment)
    except Exception:
        print('failed to create card ' + key + ' with error: ')
        traceback.print_exc(file=sys.stdout)
        sys.stdout.flush()
        return fits.Card('COMMENT', ' ' + key + ' not available', '')


def create_float_card(key, value, comment, places):
    """Create a fits.Card instance with a float value
       rounded to a specified number of significant figures
    """
    fmt = '{:8s}= {:.0' + str(places) + 'f}'
    card = fits.Card.fromstring(fmt.format(key, value))
    card.comment = comment
    return card


def header_to_dict(header):
    ret = {}
    for card in header.cards:
        if card.is_blank or not card.keyword or card.keyword == 'COMMENT':
            continue

        ret[card.keyword] = card.value
    return ret


def move_bscale_bzero(header):
    """Moves the BSCALE and BZERO header keys to the start of the header
       to be grouped with the other image keywords"""
    header.set('BSCALE', header['BSCALE'], after='NAXIS2')
    header.set('BZERO', header['BZERO'], after='BSCALE')


def add_telescope_header(header, info, log_name, telescope_cards, telescope_query_timeout):
    """Queries the telescope status and adds the appropriate header keys to the frame"""
    desc = fits.Card('COMMENT', ' ---          TELESCOPE INFORMATION          --- ')
    header.append(fits.Card(None, None), end=True)
    header.append(desc, end=True)

    data = {}
    # Query data from daemons
    for card in telescope_cards:
        key = card['daemon'].uri + '_' + card['method']
        if key in data:
            continue

        if 'camera' in card and card['camera'] != info['camera_id']:
            continue

        try:
            with card['daemon'].connect(telescope_query_timeout) as d:
                data[key] = getattr(d, card['method'])()
        except Exception as e:
            log.error(log_name, 'Failed to query telescope metadata (' + str(e) + ')')
            sys.stdout.flush()
            data[card['daemon']] = {'data': {}}

    # Build cards
    for card in telescope_cards:
        if 'camera' in card and card['camera'] != info['camera_id']:
            continue

        key = card['daemon'].uri + '_' + card['method']
        value = data.get(key, {}).get(card['parameter'], None)
        if value is not None:
            header.append(create_formatted_card(card['key'], value, card['comment'], card['type']), end=True)
        else:
            header.append(fits.Card('COMMENT', ' ' + card['key'] + ' not available', ''), end=True)


def add_environment_header(header, info, log_name, environment_cards, environment_daemon, environment_query_timeout):
    """Queries the environment status and adds the appropriate header keys to the frame"""
    desc = fits.Card('COMMENT', ' ---         ENVIRONMENT INFORMATION         --- ')
    header.append(fits.Card(None, None), end=True)
    header.append(desc, end=True)

    try:
        with environment_daemon.connect(environment_query_timeout) as environment:
            data = environment.status()
    except Exception as e:
        data = {}
        log.error(log_name, 'Failed to query environment metadata (' + str(e) + ')')
        sys.stdout.flush()

    for card in environment_cards:
        if 'camera' in card and card['camera'] != info['camera_id']:
            continue

        sensor_data = data.get(card['sensor'], {}).get('parameters', {}).get(card['parameter'], {})
        if sensor_data and sensor_data.get('current', False):
            header.append(create_formatted_card(card['key'], sensor_data['latest'],
                                                card['comment'], card['type']), end=True)
        else:
            header.append(fits.Card('COMMENT', ' ' + card['key'] + ' not available', ''), end=True)


def add_pipeline_header(header, info, filename):
    """Adds pipeline configuration to the frame header"""
    desc = fits.Card('COMMENT', ' ---               DATA PIPELINE               --- ')
    header.append(fits.Card(None, None), end=True)
    header.append(desc, end=True)

    header.append(fits.Card('DPSWVER', SOFTWARE_VERSION,
                            'data pipeline software version'), end=True)
    header.append(fits.Card('FILESAVE', filename is not None,
                            'image has been archived to disk'), end=True)

    if filename is not None:
        header.append(fits.Card('FILENAME', filename, 'archived image name'), end=True)
    else:
        header.append(fits.Card('COMMENT', ' FILENAME not available', ''))

    header.append(fits.Card('IMAGETYP', info['frame_type'], 'frame type'), end=True)

    if info['frame_type'] == 'SCIENCE':
        header.append(fits.Card('OBJECT', info['frame_object'], 'science target name'), end=True)
    else:
        header.append(fits.Card('COMMENT', ' OBJECT not available', ''))


def add_wcs_header(header, info, objects, config):
    """Solves frame WCS and adds the appropriate header keys to the frame.
       Returns a list of xpa messages that can be given to update_previews"""
    if objects is None or len(objects) <= 0:
        return []

    width = header['NAXIS1']
    height = header['NAXIS2']

    try:
        table_path = '/var/tmp/' + info['camera_id'] + '-' + str(header['EXPCNT']) + '-scratch.xyls'
        try:
            os.remove(table_path)
        except OSError:
            pass

        objects.write(table_path, format='fits')

        wcs_path = '/var/tmp/' + info['camera_id'] + '-' + str(header['EXPCNT']) + '-scratch.wcs'
        try:
            os.remove(wcs_path)
        except OSError:
            pass

        wcs_args = [
            '/usr/bin/solve-field',
            '--overwrite', '--no-plots',
            '--axy', 'none', '--rdls', 'none', '--match', 'none',
            '--corr', 'none', '--solved', 'none',
            '--scale-units', 'arcsecperpix', '--scale-high', str(config['wcs_scale_high']),
            '--scale-low', str(config['wcs_scale_low']), '--width', str(width), '--height', str(height)]

        ra = config.get('wcs_search_ra_card', None)
        dec = config.get('wcs_search_dec_card', None)
        radius = config.get('wcs_search_radius', None)
        if ra and ra in header and dec and dec in header and radius:
            wcs_args.extend([
                '--ra', header[ra],
                '--dec', header[dec],
                '--radius', str(radius)
            ])

        wcs_args.append(table_path)

        subprocess.check_call(wcs_args, timeout=config['wcs_timeout'],
                              stdout=subprocess.DEVNULL,
                              stderr=subprocess.DEVNULL)

        wcs_data = ''
        wcs_ignore_cards = ['SIMPLE', 'BITPIX', 'NAXIS', 'EXTEND', 'DATE', 'IMAGEW', 'IMAGEH']
        comment = 'astrometry.net wcs solution'
        with open(wcs_path, encoding='ascii') as wcs_file:
            h = wcs_file.read()
            # ds9 will only accept newline-delimited keys
            # so we need to reformat the 80-character cards
            for line in [h[i:i+80] for i in range(0, len(h), 80)]:
                key = line[0:8].strip()
                if '=' in line and key not in wcs_ignore_cards:
                    card = fits.Card.fromstring(line)
                    value = line[9:].split('/')[0].strip()
                    wcs_data += key + ' = ' + value + '\n'

                    header.append(fits.Card(card.keyword, card.value, comment), end=True)
        annotations = [
            ('wcs replace', wcs_data),
            ('regions', ANNOTATION_LABEL.format(width / 5, -config['preview_ds9_annotation_margin'],
                                                'WCS: SOLVED'))
        ]

        if info['rotation_enabled']:
            wcs = WCS(header)
            coords = wcs.all_pix2world([width // 2, width // 2], [height // 2, height // 2 + 1], 0)
            north_angle = np.degrees(np.arctan2(np.diff(coords[1]), np.diff(coords[0]))).item()
            annotations.append(('regions', ANNOTATION_LABEL.format(
                width / 5, -2 * config['preview_ds9_annotation_margin'],
                f'ROT: North is {north_angle:.1f} deg CCW of +X')))

        return annotations
    except Exception:
        print('failed to update wcs with error: ')
        traceback.print_exc(file=sys.stdout)
        sys.stdout.flush()

        return [
            ('regions', ANNOTATION_LABEL.format(width / 4, -config['preview_ds9_annotation_margin'], 'WCS: FAILED'))
        ]


def detect_objects(header, data, background_rms, config, crop_x, crop_y):
    """returns an Astropy table containing (X, Y, FLUX, HFD_PX, HFD_ARCSEC) for each
       object detected by sep"""
    try:
        platescale = config['platescale']
        ccd_bin = config.get('ccd_bin_card', None)
        if ccd_bin and ccd_bin in header:
            platescale *= header[ccd_bin]

        # Note: data is expected to be background subtracted
        objects = sep.extract(data, 5 * background_rms)

        # Work around sep issue #110
        objects['theta'][objects['theta'] > np.pi / 2] -= np.pi / 2

        kronrad, kron_flag = sep.kron_radius(data, objects['x'], objects['y'],
                                             objects['a'], objects['b'], objects['theta'], 6.0)

        flux, _, flux_flag = sep.sum_ellipse(data, objects['x'], objects['y'], objects['a'], objects['b'],
                                             objects['theta'], 2.5 * kronrad, subpix=0)

        r, radius_flag = sep.flux_radius(data, objects['x'], objects['y'],
                                         6.0 * objects['a'], 0.5, normflux=flux, subpix=5)

        # pylint: disable=no-member
        filt = np.logical_and.reduce([
            objects['npix'] >= config['object_minpix'],
            kron_flag == 0,
            flux_flag == 0,
            radius_flag == 0
        ])
        # pylint: enable=no-member

        objects = Table(
            [objects['x'][filt] + crop_x, objects['y'][filt] + crop_y, flux[filt],
             2 * r[filt], 2 * r[filt] * platescale],
            names=('X', 'Y', 'FLUX', 'HFD_PX', 'HFD_ARCSEC'),
            dtype=(float, float, float, float, float)
        )

        objects.sort('FLUX', reverse=True)
        return objects
    except Exception:
        print('failed to extract objects with error: ')
        traceback.print_exc(file=sys.stdout)
        sys.stdout.flush()
        return Table(names=('X', 'Y', 'FLUX', 'HFD_PX', 'HFD_ARCSEC'), dtype=(float, float, float, float, float))


def add_hfd_header(header, objects):
    """Adds the median hfd to the frame header"""
    try:
        if objects is None or len(objects) <= 0:
            return

        header.append(create_float_card('MEDHFD', np.median(objects['HFD_ARCSEC']).item(),
                                        '[arcsec] median estimated half-flux diameter', 1), end=True)
        header.append(fits.Card('HFDCNT', len(objects),
                                'number of sources used to estimate the HFD'), end=True)
    except Exception:
        print('failed to calculate HFD with error:')
        traceback.print_exc(file=sys.stdout)
        sys.stdout.flush()


def add_intensity_stats_header(header, data_raw, data_cropped, config):
    """Adds the frame intensity to the frame header"""
    try:
        header.append(create_float_card('MEANCNTS', np.mean(data_cropped),
                                        '[adu] mean frame counts', 2), end=True)
        header.append(create_float_card('MEDCNTS', np.median(data_cropped),
                                        '[adu] median frame counts', 2), end=True)
        header.append(create_float_card('STDCNTS', np.std(data_cropped),
                                        '[adu] frame counts standard deviation', 2), end=True)

        if 'overscan_region_card' in config and config['overscan_region_card'] in header:
            bias, *_ = window_header_region(header, data_raw, config['overscan_region_card'])
            header.append(create_float_card('MEANBIAS', np.mean(bias),
                                            '[adu] mean overscan bias counts', 2), end=True)
            header.append(create_float_card('MEDBIAS', np.median(bias),
                                            '[adu] median overscan bias counts', 2), end=True)
            header.append(create_float_card('STDBIAS', np.std(bias),
                                            '[adu] overscan bias standard deviation', 2), end=True)

    except Exception:
        print('failed to calculate intensity stats with error: ')
        traceback.print_exc(file=sys.stdout)
        sys.stdout.flush()


def generate_header_annotations(header, info, config):
    """Returns the XPA commands to display the time and exposure on the live previews"""
    width = header['NAXIS1']
    height = header['NAXIS2']
    timestamp = header['DATE-OBS']
    time_src = header.get('TIME-SRC', None)
    if time_src:
        timestamp += f' ({time_src})'

    exptime = f'{info["camera_id"]} @ {header["EXPTIME"]:.3f}s'
    cam_transfer = header.get('CAM-TFER', None)
    if cam_transfer:
        exptime += f' ({cam_transfer})'

    saved = 'SAVED' if 'FILENAME' in header else 'NOT SAVED'
    annotations = [
        ('regions', ANNOTATION_LABEL.format(width / 4, height + config['preview_ds9_annotation_margin'], timestamp)),
        ('regions', ANNOTATION_LABEL.format(3 * width / 4, height + config['preview_ds9_annotation_margin'], exptime)),
        ('regions', ANNOTATION_LABEL.format(width / 2, -config['preview_ds9_annotation_margin'], saved))
    ]

    if 'MEDHFD' in header:
        hfd_label = ANNOTATION_LABEL.format(
            4 * width / 5, -config['preview_ds9_annotation_margin'],
            'HFD: ' + str(header['MEDHFD']) + ' arcsec')
        annotations.append(('regions', hfd_label))

    return annotations


def generate_hfd_grid_annotations(header, objects, config):
    """Returns the XPA commands to display the hfd grid overlay on the live previews"""
    x_step = header['NAXIS1'] / config['hfd_grid_tiles_x']
    y_step = header['NAXIS2'] / config['hfd_grid_tiles_y']

    annotations = []
    for j in range(config['hfd_grid_tiles_y']):
        for i in range(config['hfd_grid_tiles_x']):
            # pylint: disable=no-member
            filt = np.logical_and.reduce([
                x_step * i < objects['X'],
                objects['X'] < x_step * (i + 1),
                y_step * j < objects['Y'],
                objects['Y'] < y_step * (j + 1)
            ])
            # pylint: enable=no-member

            x = (i + 0.5) * x_step
            y = (j + 0.5) * y_step
            median = np.median(objects['HFD_ARCSEC'][filt])
            annotations.append(('regions', f'box({x:.6f},{y:.6f},{x_step},{y_step}) # color=yellow'))
            annotations.append(('regions', f'text {x:.6f} {y:.6f} # color=green text="{median:.2f}"'))

    return annotations


def update_dashboard(header, data, config):
    try:
        with tempfile.TemporaryDirectory() as path:
            size = np.shape(data)
            metadata = {
                'date': header['DATE-OBS'],
                'exptime': header['EXPTIME'],
                'saved': header['FILESAVE'],
                'filename': header['FILENAME'] if 'FILENAME' in header else '',
                'size': [size[1], size[0]],
                'clipsize': config['dashboard_clip_size'],
            }

            scaled = rescale_image_data(data, config['dashboard_min_threshold'], config['dashboard_max_threshold'])
            preview = Image.fromarray(scaled).convert('RGB')
            if config['dashboard_flip_vertical']:
                preview = ImageOps.flip(preview)
            if config['dashboard_flip_horizontal']:
                preview = ImageOps.mirror(preview)

            preview.thumbnail((config['dashboard_thumb_size'], config['dashboard_thumb_size']))
            thumb_path = os.path.join(path, config['dashboard_prefix']+ '-thumb.jpg')
            preview.save(thumb_path, 'JPEG', quality=80, optimize=True, progressive=True, clobber=True)

            # Clip the central region from the image
            cx1 = (data.shape[0] - config['dashboard_clip_size']) // 2
            cx2 = cx1 + config['dashboard_clip_size']
            cy1 = (data.shape[1] - config['dashboard_clip_size']) // 2
            cy2 = cy1 + config['dashboard_clip_size']

            clipped = rescale_image_data(data[cx1:cx2, cy1:cy2],
                                         config['dashboard_min_threshold'],
                                         config['dashboard_max_threshold'])
            preview = Image.fromarray(clipped).convert('RGB')
            preview = ImageOps.flip(preview)
            clip_path = os.path.join(path, config['dashboard_prefix'] + '-clip.jpg')
            preview.save(clip_path, 'JPEG', quality=40, optimize=True, progressive=True, clobber=True)

            json_path = os.path.join(path, config['dashboard_prefix'] + '.json')
            with open(json_path, 'w', encoding='utf-8') as outfile:
                json.dump(metadata, outfile)

            with open(f'/home/{config["dashboard_user"]}/.ssh/{config["dashboard_key"]}', encoding='utf-8') as keyfile:
                key = RSAKey.from_private_key(keyfile)

            with SSHClient() as ssh:
                ssh.load_system_host_keys(f'/home/{config["dashboard_user"]}/.ssh/known_hosts')
                ssh.connect(getattr(IP, config['dashboard_remote_host']),
                            username=config['dashboard_remote_user'], pkey=key)

                with SCPClient(ssh.get_transport()) as scp:
                    scp.put(thumb_path, remote_path=config['dashboard_remote_path'])
                    scp.put(clip_path, remote_path=config['dashboard_remote_path'])
                    scp.put(json_path, remote_path=config['dashboard_remote_path'])

    except Exception:
        print('failed to generate dashboard preview with error:')
        traceback.print_exc(file=sys.stdout)
        sys.stdout.flush()


def notify_opsd(header, ops_daemon, log_name):
    try:
        with ops_daemon.connect() as ops:
            ops.notify_processed_frame(header_to_dict(header))

    except Exception as e:
        log.error(log_name, 'Failed to notify operations daemon of completed frame (' + str(e) + ')')
        sys.stdout.flush()


def notify_guide_profiles(header, data, log_name, ops_daemon):
    # Collapse into x/y profiles
    # Note: data is expected to already be background-subtracted
    # Note: Pyro doesn't support numpy arrays, so convert to a built-in type
    profile_x = array.array('f', np.mean(data, axis=0))
    profile_y = array.array('f', np.mean(data, axis=1))

    try:
        with ops_daemon.connect() as ops:
            ops.notify_guide_profiles(header_to_dict(header), profile_x, profile_y)

    except Exception as e:
        log.error(log_name, 'Failed to notify operations daemon of guide profiles (' + str(e) + ')')


def process_previews(preview_queue, ds9_preview_ports, ds9_preview_xpa_lock):
    """A thread running in each processing process that allows ds9 preview updates
       to run concurrently with the main processing work.
     """
    while True:
        # Block until a preview is available
        data, header, annotations = preview_queue.get()

        with ds9_preview_xpa_lock:
            for i, port in enumerate(ds9_preview_ports):
                if port == 0:
                    continue

                try:
                    p = pyds9.DS9('7f000001:' + str(port), start=False, wait=1)
                    if data is not None:
                        hdu = fits.PrimaryHDU(data, header=header)
                        move_bscale_bzero(hdu.header)
                        p.set_pyfits(fits.HDUList(hdu))

                    for annotation in annotations:
                        if isinstance(annotation, tuple):
                            p.set(annotation[0], annotation[1])
                        else:
                            p.set(annotation)
                except Exception as e:
                    socket_path = '/tmp/pipelined-' + str(port)
                    if os.path.exists(socket_path):
                        subprocess.check_call([
                            'ssh',
                            '-S', socket_path,
                            '-O', 'exit',
                            'localhost'
                        ])

                    ds9_preview_ports[i] = 0
                    print(f'unregistering preview {port}: {e}', flush=True)


def process_frames(processing_queue, camera_config,
                   preview_timestamp, ds9_preview_ports, ds9_preview_xpa_lock,
                   guide_timestamp, guide_min_interval,
                   dashboard_timestamp, log_name,
                   environment_cards, environment_daemon_name, environment_query_timeout,
                   telescope_cards, telescope_query_timeout,
                   ops_daemon_name):
    """
    Helper process to add frame metadata and calculate image details for the operations daemon.
    This uses a process (rather than a thread) to avoid the GIL bottlenecking throughput,
    and multiple worker processes allow frames to be processed in parallel.
    """
    environment_daemon = getattr(daemons, environment_daemon_name)
    ops_daemon = getattr(daemons, ops_daemon_name)

    # Update previews on a separate thread to avoid blocking
    preview_queue = queue.Queue()
    threading.Thread(target=process_previews, daemon=True,
                     args=(preview_queue, ds9_preview_ports, ds9_preview_xpa_lock)
                     ).start()

    while True:
        # Blocks until a frame is available for processing
        info = processing_queue.get()
        loadpath = os.path.join(camera_config['input_data_path'], info['filename'])

        try:
            start = datetime.datetime.utcnow()

            # We only update the header, not the data.
            # Reduce the overheads from astropy fits by reading what we need at the start,
            # manipulating just the headers in isolation, and then doing a direct byte
            # replacement of the header in the original file.
            # This relies on the camera daemon reserving sufficient blank header keywords
            # to be replaced below. This avoids having to move all the pixel data
            # in the file to create additional header blocks.
            header = fits.getheader(loadpath)
            steps = ['headers']

            # We can skip loading the frame data if we know for sure that
            # we aren't going to need it.
            data_bgsubtracted = data_cropped = data_raw = data_background = None
            crop_x = crop_y = 0
            header_time = Time(header['DATE-OBS'])

            guide_profiles_stale = False
            if info['guide_profiles']:
                with guide_timestamp.get_lock():
                    if header_time > Time(guide_timestamp.value.decode('ascii')) + guide_min_interval * u.s:
                        guide_timestamp.value = header['DATE-OBS'].encode('ascii')
                        guide_profiles_stale = True

            preview_stale = False
            if any(ds9_preview_ports):
                with preview_timestamp.get_lock():
                    next_preview_time = Time(preview_timestamp.value.decode('ascii')) + \
                        camera_config['preview_min_interval'] * u.s
                    if header_time > next_preview_time:
                        preview_timestamp.value = header['DATE-OBS'].encode('ascii')
                        preview_stale = True

            dashboard_stale = False
            if info['dashboard_enabled']:
                with dashboard_timestamp.get_lock():
                    next_dashboard_time = Time(dashboard_timestamp.value.decode('ascii')) + \
                        camera_config['dashboard_min_interval'] * u.s
                    if header_time > next_dashboard_time:
                        dashboard_timestamp.value = header['DATE-OBS'].encode('ascii')
                        dashboard_stale = True

            calculate_objects = info['hfd_enabled'] or info['hfd_grid_enabled'] or \
                info['wcs_enabled'] or info['rotation_enabled']

            calculate_bgsubtracted = calculate_objects or guide_profiles_stale
            calculate_cropped = calculate_bgsubtracted or info['intensity_stats_enabled'] or dashboard_stale
            if calculate_cropped or preview_stale:
                data_raw, header = fits.getdata(loadpath, header=True)
                steps.append('data')
            else:
                header = fits.getheader(loadpath)

            # Strip empty rows from the end of the header
            # fits blocks are 2880 bytes, which can hold 36 80 byte header cards.
            header_block_capacity = (len(header) + 35) // 36
            while header.cards[-1][0] == '' and header.cards[-1][1] == '':
                header.pop()

            if calculate_cropped:
                data_cropped, crop_x, crop_y = window_header_region(header, data_raw,
                                                                    camera_config.get('image_region_card', None))
                steps.append('crop')

            if calculate_bgsubtracted:
                data_bgsubtracted = data_cropped.astype(float)
                data_background = sep.Background(data_bgsubtracted)
                data_background.subfrom(data_bgsubtracted)
                steps.append('background')

            if guide_profiles_stale:
                notify_guide_profiles(header, data_bgsubtracted, log_name, ops_daemon)
                steps.append('guiding')

            output_filename = None
            if info['output_archive']:
                date = datetime.datetime.strptime(header['DATE-OBS'], '%Y-%m-%dT%H:%M:%S.%f')
                output_filename = info['output_prefix'] + '-' + info['camera_id'] + '-' + \
                    date.strftime('%Y%m%d%H%M%S%f')[:-3] + '.fits'

            move_bscale_bzero(header)
            add_telescope_header(header, info, log_name, telescope_cards, telescope_query_timeout)
            add_environment_header(header, info, log_name, environment_cards,
                                   environment_daemon, environment_query_timeout)
            add_pipeline_header(header, info, output_filename)

            if info['intensity_stats_enabled']:
                add_intensity_stats_header(header, data_raw, data_cropped, camera_config)
                steps.append('intstats')

            objects = None
            if calculate_objects:
                objects = detect_objects(header, data_bgsubtracted, data_background.globalrms, camera_config,
                                         crop_x, crop_y)
                steps.append('objdetect')

            if info['hfd_enabled']:
                add_hfd_header(header, objects)
                steps.append('hfd')

            # Update previews before the relatively-slow WCS step
            if preview_stale:
                annotations = generate_header_annotations(header, info, camera_config)
                preview_queue.put((data_raw, header, annotations))

            if info['wcs_enabled'] or info['rotation_enabled']:
                wcs = add_wcs_header(header, info, objects, camera_config)
                if preview_stale:
                    preview_queue.put((None, header, wcs))
                steps.append('wcs')
                if info['rotation_enabled']:
                    steps.append('rotation')

            if info['hfd_grid_enabled']:
                grid = generate_hfd_grid_annotations(header, objects, camera_config)
                if preview_stale:
                    preview_queue.put((None, header, grid))
                steps.append('hfd_grid')

            notify_opsd(header, ops_daemon, log_name)

            if output_filename:
                # Replace header directly in the input file before moving to the final output location
                block_count = (len(header) + 35) // 36
                if block_count > header_block_capacity:
                    # Not enough space to fit all the keys!
                    truncated = 0
                    while len(header) > 36 * header_block_capacity - 1:
                        truncated += 1
                        header.pop()
                    header.append(fits.Card('COMMENT', f' WARNING: {truncated} header cards truncated', ''))

                # It is very likely that we will have more than 36 cards of unused capacity if WCS solutions
                # are disabled. The fits specification requires that the END card is within the block immediately
                # before the data starts, so pad the header with blank cards to do this.
                padding = max(0, 36 * (header_block_capacity - 1) - len(header))
                for _ in range(padding):
                    header.append(fits.Card(None, None))

                with open(loadpath, 'r+b') as f:
                    f.write(header.tostring().encode('ascii'))

                savepath = os.path.join(camera_config['output_data_path'], info['output_subdirectory'], output_filename)
                shutil.move(loadpath, savepath)

                log.info(log_name, 'Saved frame ' + output_filename)
                sys.stdout.flush()

            process_time = round((datetime.datetime.utcnow() - start).total_seconds(), 1)
            print('processed ' + loadpath + ' in ' + str(process_time) + 's (' + ', '.join(steps) + ')', flush=True)

            if dashboard_stale:
                update_dashboard(header, data_cropped, camera_config)

        except Exception:
            print('Unexpected exception while processing ' + loadpath + ': ')
            traceback.print_exc(file=sys.stdout)
            sys.stdout.flush()
        finally:
            try:
                if os.path.exists(loadpath):
                    os.remove(loadpath)
            except Exception:
                print('Failed to delete temporary frame: ' + loadpath, flush=True)


class PipelineWorkerDaemon:
    """Camera-specific pipeline worker"""
    def __init__(self, config, camera_id):
        self._config = config
        self._camera_config = config.cameras[camera_id]
        self._camera_daemon = getattr(daemons, self._camera_config['worker_daemon'])

        # Block if all workers are saturated so the pyro notification
        # can timeout and propagate back to stop the camera exposures
        self._processing_queue = multiprocessing.Queue(maxsize=self._camera_config['worker_processes'])

        # Avoid race conditions between worker processes
        start_timestamp = Time.now().strftime('%Y-%m-%dT%H:%M:%S.%f').encode('ascii')
        self._preview_timestamp = multiprocessing.Array('c', start_timestamp)
        self._guide_timestamp = multiprocessing.Array('c', start_timestamp)
        self._dashboard_timestamp = multiprocessing.Array('c', start_timestamp)

        # Use 0 to indicate an unregistered preview slot
        self._ds9_preview_ports = multiprocessing.Array('I', [0] * self._camera_config['preview_max_instances'])
        self._ds9_preview_registration_lock = threading.Lock()
        self._ds9_preview_xpa_lock = multiprocessing.Lock()

        for _ in range(self._camera_config['worker_processes']):
            multiprocessing.Process(target=process_frames, daemon=True, args=(
                self._processing_queue, self._camera_config,
                self._preview_timestamp, self._ds9_preview_ports, self._ds9_preview_xpa_lock,
                self._guide_timestamp, config.guiding_min_interval,
                self._dashboard_timestamp, config.log_name,
                config.environment_cards, config.environment_daemon_name, config.environment_query_timeout,
                config.telescope_cards, config.telescope_query_timeout,
                config.ops_daemon_name)).start()

    @Pyro4.expose
    def create_output_subdirectory(self, name):
        """Attempts to set the output data subdirectory inside the output_data_path camera config variable
           The given directory name will be created and/or checked to ensure it is writable.
           Returns a CommandStatus indicating the result
        """
        if not pyro_client_matches([self._config.daemon.host]):
            return CommandStatus.InvalidControlIP

        path = os.path.join(self._camera_config['output_data_path'], name)
        try:
            os.makedirs(path, exist_ok=True)
        except FileExistsError:
            pass
        except Exception:
            print('failed to create night dir with exception:')
            traceback.print_exc(file=sys.stdout)
            return CommandStatus.NewDirectoryFailed

        # Throws on error
        try:
            with tempfile.TemporaryFile(dir=path):
                return CommandStatus.Succeeded
        except Exception:
            return CommandStatus.DirectoryNotWritable

    @Pyro4.expose
    def notify_frame(self, info):
        """Called by the master pipeline daemon to notify that a new frame is ady for processing.
           filename is specified relative to the incoming_data_path config value."""
        if not pyro_client_matches([self._config.daemon.host]):
            return

        self._processing_queue.put(info)

    @Pyro4.expose
    def register_preview(self, host, port):
        """Register a ds9 window for previews"""
        if not pyro_client_matches([self._config.daemon.host]):
            return CommandStatus.InvalidControlIP

        with self._ds9_preview_registration_lock:
            total = len(self._ds9_preview_ports)
            for i in range(total + 1):
                if i == total:
                    return CommandStatus.TooManyPreviews

                if self._ds9_preview_ports[i] == 0:
                    break

            if host != self._camera_daemon.host:
                try:
                    # pyds9 will fail unless the local and remote ports are the same, so we need
                    # the same port to be free on both client and server.
                    # This check may race against another process stealing the port, but this
                    # should be rare enough that the user can just run the preview command again.
                    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                        if s.connect_ex(('localhost', port)) == 0:
                            return CommandStatus.TunnelPortInUse

                    # Create a ssh tunnel using multiplexing to simplify closing it later
                    # Inspired by https://stackoverflow.com/questions/4364355/how-to-open-an-ssh-tunnel-using-python
                    # This requires key-based ssh access has been configured from the server to the calling client!
                    socket_path = '/tmp/pipelined-' + str(port)
                    subprocess.check_call([
                        'ssh', '-MfN',
                        '-S', socket_path,
                        '-L', f'{port}:127.0.0.1:{port}',
                        '-o', 'ExitOnForwardFailure=yes',
                        host
                    ])

                    subprocess.check_call([
                        'ssh',
                        '-S', socket_path,
                        '-O', 'check',
                        'localhost'
                    ])
                except Exception:
                    return CommandStatus.TunnelCreationFailed

            self._ds9_preview_ports[i] = port
        return CommandStatus.Succeeded


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Data pipeline worker daemon')
    parser.add_argument('config', help='Path to configuration json file')
    parser.add_argument('camera_id', help='Camera ID')
    args = parser.parse_args()
    c = Config(args.config, validate_camera=args.camera_id)
    daemon = getattr(daemons, c.cameras[args.camera_id]['worker_daemon'])
    daemon.launch(PipelineWorkerDaemon(c, args.camera_id))
