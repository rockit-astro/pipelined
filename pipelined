#!/usr/bin/env python3
#
# This file is part of pipelined.
#
# pipelined is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# pipelined is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with pipelined.  If not, see <http://www.gnu.org/licenses/>.

"""Daemon for the Warwick one-metre telescope frame pipeline"""

# pylint: disable=too-few-public-methods
# pylint: disable=no-self-use
# pylint: disable=broad-except
# pylint: disable=invalid-name
# pylint: disable=too-many-instance-attributes
# pylint: disable=too-many-locals

import datetime
import json
import math
import os
import pathlib
from PIL import Image, ImageOps
import queue
import re
import shutil
import subprocess
import tempfile
import threading
import numpy
import sep
from astropy.io import fits
import astropy.table
import pyds9
import Pyro4
import warwick.observatory as observatory
import warwick.observatory.helpers as helpers
from warwick.observatory import TryLock

# Set automatically when generating RPM package
SOFTWARE_VERSION = 'UNKNOWN'

# Maximum time to block the camera waiting to hand a frame to the processing thread
MAX_NOTIFY_WAIT = 10

# Timeout for telescope / environment status queries
STATUS_QUERY_TIMEOUT = 1

PIXELSHIFT_PATH = '/home/saft/src/pixelshift/pixelshift'

GUIDE_REFERENCE_FRAME_PATH = '/var/tmp/guide_reference.fits'

# Matrix coefficients transforming pixel offsets to sky offsets.
# These are just the [CD1_1, CD1_2, CD2_1, CD2_2] wcs parameters
GUIDE_TRANSFORM_MATRICES = {
    'BLUE': [0.000108685545277, -3.58470982799E-07,
             -2.98097156944E-07, -0.000108607868069],
    'RED': [-0.00010858955941, 1.83797853216E-07,
            -2.66674559873E-07, -0.000108601351771]
}

CAMERA_URLS = {
    'BLUE': observatory.daemons.onemetre_blue_camera,
    'RED': observatory.daemons.onemetre_red_camera
}

CAMERAS = ['BLUE', 'RED']

GUIDE_FUDGE_FACTOR = 0.5

# Time limit (seconds) allowed to solve the WCS coordinates of a preview frame
WCS_LIMIT = 4.0

# Maximum field size (arcmin)
# Note that this needs to account for the overscan too!
WCS_SCALE_HIGH = 13.7

# Maximum tcs ra/dec error (degrees)
WCS_SEARCH_RADIUS = 1.75

# Unbinned arcseconds per pixel
OBJECT_PLATESCALE = 0.391

# Minimum number of pixels for considering a source for FWHM estimation
OBJECT_MINPIX = 16

# Top/bottom margin (in px) to offset the annotation labels
ANNOTATION_LABEL_MARGIN = 30

ANNOTATION_LABEL = 'image; text {} {} #color=green font="helvetica 12 bold" text="{}" select=0'

# This should be kept in sync with the dictionary in pipeline
class CommandStatus:
    """Numeric return codes"""
    # General error codes
    Succeeded = 0
    Failed = 1
    Blocked = 2

    DirectoryNotWritable = 50

def rescale_image_data(data, clip_low, clip_high):
    """ Returns a normalised array where clip_low percent of the pixels are 0 and
        clip_high percent of the pixels are 255
    """
    high = numpy.percentile(data, clip_high)
    low = numpy.percentile(data, clip_low)
    scale = 255. / (high - low)
    data = numpy.clip(data, low, high)
    return scale * (data - low)

class PipelineDaemon:
    """Daemon interface for data pipeline"""
    def __init__(self):
        self._command_lock = threading.Lock()
        self._guide_condition = threading.Condition()
        self._guide_camera_id = None
        self._guide_latest_frame = None
        self._guide_x_binning = 1
        self._guide_y_binning = 1
        self._guide_cos_dec = 1
        self._guide_background_tile_size = 0
        self._guide_reference_frame = None
        self._guide_last_guide_frame = None

        guide_thread = threading.Thread(target=self.__process_guide_frames)
        guide_thread.daemon = True
        guide_thread.start()

        self._process_queue = {}
        for cam in CAMERAS:
            self._process_queue[cam] = queue.Queue(maxsize=1)
            thread = threading.Thread(target=self.__process_frames, args=[cam])
            thread.daemon = True
            thread.start()

        self._wcs_enabled = False
        self._fwhm_enabled = False
        self._intensity_stats_enabled = False
        self._dashboard_enabled = True
        self._compression_enabled = False

        # Information for building the output filename
        self._output_directory = pathlib.Path('/home/saft/OBS_DATA/')
        self._output_frame_prefix = 'unknown'

        self._frame_object = ''
        self._frame_type = 'JUNK'

        self._output_frame_number = {}
        self._output_save_to_disk = {}
        self._ds9_preview_addresses = {}
        for cam in CAMERAS:
            self._output_frame_number[cam] = 0
            self._output_save_to_disk[cam] = False
            self._ds9_preview_addresses[cam] = []

        # Min and max percentage thresholds to use for the preview pngs
        self._dashboard_min_threshold = 5
        self._dashboard_max_threshold = 95
        self._dashboard_thumb_size = 512

    def __process_guide_frames(self):
        """Calculates image shifts between frames and offsets telescope to compensate"""
        while True:
            # Block until a frame is available for processing
            frame = None
            with self._guide_condition:
                while self._guide_latest_frame is None:
                    self._guide_condition.wait()

                frame = self._guide_latest_frame
                self._guide_latest_frame = None

            # New guide session: take a copy of this frame to use as the reference
            if self._guide_reference_frame is None:
                print('taking', frame, 'as new guide reference')
                observatory.log.info('pipelined', 'Initializing autoguider')
                try:
                    self._guide_reference_frame = GUIDE_REFERENCE_FRAME_PATH
                    shutil.copy(frame, GUIDE_REFERENCE_FRAME_PATH)
                    reference = fits.open(GUIDE_REFERENCE_FRAME_PATH)
                    self._guide_x_binning = reference[0].header['CCD-XBIN']
                    self._guide_y_binning = reference[0].header['CCD-YBIN']

                    # RA becomes compressed away from the equator
                    # Calculate the correction scale factor
                    dec = reference[0].header['TELDEC']
                    parts = dec.split(':')

                    # Sign doesn't matter, so discard immediately
                    a = abs(float(parts[0]))
                    b = float(parts[1])
                    c = float(parts[2])
                    self._guide_cos_dec = math.cos((a + b / 60 + c / 3600) * math.pi / 180)

                    reference.close()
                except Exception as e:
                    print('failed with error: ' + str(e))
                    observatory.log.error('pipelined', 'Failed to initialize autoguider (' \
                                          + str(e) + ')')
                continue

            if self._guide_camera_id not in GUIDE_TRANSFORM_MATRICES:
                print('no transform matrix for camera ' + self._guide_camera_id)
                continue

            try:
                # TODO: use Donuts
                output = subprocess.check_output(
                    [PIXELSHIFT_PATH, frame, self._guide_reference_frame,
                     str(self._guide_background_tile_size)], universal_newlines=True, timeout=5)

                t = GUIDE_TRANSFORM_MATRICES[self._guide_camera_id]
                coords = output.split(' ')

                # Measured offset in px
                dx = float(coords[0])
                dy = float(coords[1])

                # First-order approximated offset in ra, dec (degrees)
                dra = (t[0] * dx + t[1] * dy) * self._guide_x_binning / self._guide_cos_dec
                ddec = (t[2] * dx + t[3] * dy) * self._guide_y_binning

                # Calculate actual telescope offset by running offsets through PID loop
                # TODO: This currently only accounts for proportional offsets (no history)
                guide_offset_ra = GUIDE_FUDGE_FACTOR * dra * math.pi / 180
                guide_offset_dec = GUIDE_FUDGE_FACTOR * ddec * math.pi / 180

                # TODO: Check for and drop anomalously large offsets

                print('offset px: {} {}; arcsec: {} {}'.format(dx, dy, 3600 * dra, 3600 * ddec))

                with observatory.daemons.onemetre_telescope.connect() as teld:
                    teld.offset_radec_guiding(guide_offset_ra, guide_offset_dec)
            except Exception as e:
                print('failed to update guiding with error: ' + str(e))
                observatory.log.error('pipelined', 'Failed to calculate guide offset (' \
                                      + str(e) + ')')

    def __add_telescope_header(self, frame):
        """Queries the telescope status and adds the appropriate header keys to the frame"""
        h = frame.header
        desc = fits.Card('COMMENT', ' ---          TELESCOPE INFORMATION          --- ')
        h.append(fits.Card(None, None), end=True)
        h.append(desc, end=True)

        try:
            with observatory.daemons.onemetre_telescope.connect(STATUS_QUERY_TIMEOUT) as teld:
                t = teld.report_status()
                h.append(fits.Card('TELSWVER', t['software_version'],
                                   'tcs server software version'), end=True)
                h.append(fits.Card('TELSTATE', t['state_label'], 'telescope status'), end=True)
                if t['state'] != 0:
                    harad = t['lst'] - t['ra']
                    while harad > math.pi:
                        harad -= 2*math.pi
                    while harad < -math.pi:
                        harad += 2*math.pi

                    ra = helpers.sexagesimal(t['ra'] * 12 / math.pi)
                    h.append(fits.Card('TELRA', ra, 'telescope nominal J2000 RA'), end=True)
                    dec = helpers.sexagesimal(math.degrees(t['dec']))
                    h.append(fits.Card('TELDEC', dec, 'telescope nominal J2000 Dec'), end=True)
                    ha = helpers.sexagesimal(harad * 12 / math.pi)
                    h.append(fits.Card('TELHA', ha, 'telescope nominal HA'), end=True)
                    rad = round(math.degrees(t['ra']), 3)
                    h.append(fits.Card('TELRAD', rad, '[deg] telescope nominal J2000 RA'), end=True)
                    decd = round(math.degrees(t['dec']), 3)
                    h.append(fits.Card('TELDECD', decd, '[deg] telescope nominal J2000 Dec'),
                             end=True)
                    had = round(math.degrees(harad), 3)
                    h.append(fits.Card('TELHAD', had, '[deg] telescope nominal HA'), end=True)
                    altd = round(math.degrees(t['alt']), 3)
                    h.append(fits.Card('ALTITUDE', altd, '[deg] telescope altitude'), end=True)
                    azd = round(math.degrees(t['az']), 3)
                    h.append(fits.Card('AZIMUTH', azd, '[deg] telescope azimuth'), end=True)
                    h.append(fits.Card('TELFOCUS', t['telescope_focus_um'],
                                       '[um] telescope nominal focus'), end=True)

                if 'site_latitude' in t:
                    lat = helpers.sexagesimal(math.degrees(t['site_latitude']))
                    h.append(fits.Card('SITELAT', lat, 'telescope latitude (north)'), end=True)
                    lon = helpers.sexagesimal(math.degrees(t['site_longitude']))
                    h.append(fits.Card('SITELONG', lon, 'telescope longitude (east)'), end=True)
                    elevation = round(t['site_elevation'], 1)
                    h.append(fits.Card('SITEELEV', elevation, '[m] telescope elevation'), end=True)
        except Exception as e:
            print('failed to add telescope headers with error: ' + str(e))
            observatory.log.error('pipelined', 'Failed to query telescope metadata (' \
                                  + str(e) + ')')
        return h

    def __add_environment_header(self, frame):
        """Queries the environment status and adds the appropriate header keys to the frame"""
        h = frame.header
        desc = fits.Card('COMMENT', ' ---         ENVIRONMENT INFORMATION         --- ')
        h.append(fits.Card(None, None), end=True)
        h.append(desc, end=True)

        try:
            with observatory.daemons.onemetre_vaisala.connect(STATUS_QUERY_TIMEOUT) as vaisala:
                v = vaisala.last_measurement()
                if v is not None:
                    h.append(fits.Card('VSWVER', v['software_version'],
                                       'vaisala server software version'), end=True)
                if v is not None and v['wind_speed_valid']:
                    h.append(fits.Card('VWINDSPD', v['wind_speed'],
                                       '[km/h] vaisala wind speed'), end=True)
                if v is not None and v['wind_direction_valid']:
                    h.append(fits.Card('VWINDDIR', v['wind_direction'],
                                       '[deg] vaisala wind direction'), end=True)
                if v is not None and v['temperature_valid']:
                    h.append(fits.Card('VEXTTEMP', v['temperature'],
                                       '[deg c] vaisala external temperature'), end=True)
                if v is not None and v['relative_humidity_valid']:
                    h.append(fits.Card('VEXTHUMD', v['relative_humidity'],
                                       '[%] vaisala external humidity'), end=True)
                if v is not None and v['pressure_valid']:
                    h.append(fits.Card('VPRESSUR', v['pressure'],
                                       '[hPa] vaisala air pressure'), end=True)
        except Exception as e:
            print('failed to add vaisala headers with error: ' + str(e))
            observatory.log.error('pipelined', 'Failed to query vaisala metadata (' \
                                  + str(e) + ')')

        try:
            with observatory.daemons.onemetre_roomalert.connect(STATUS_QUERY_TIMEOUT) as roomalert:
                r = roomalert.last_measurement()
                if r is not None:
                    h.append(fits.Card('RASWVER', r['software_version'],
                                       'room alert server software version'), end=True)
                    h.append(fits.Card('DOMETEMP', r['internal_temp'],
                                       '[deg c] dome temperature'), end=True)
                    h.append(fits.Card('DOMEHUMD', r['internal_humidity'],
                                       '[%] dome humidity'), end=True)
                    h.append(fits.Card('TRUSTEMP', r['truss_temp'],
                                       '[deg c] truss temperature'), end=True)
        except Exception as e:
            print('failed to add roomalert headers with error: ' + str(e))
            observatory.log.error('pipelined', 'Failed to query roomalert metadata (' \
                                  + str(e) + ')')

        try:
            with observatory.daemons.superwasp_log.connect(STATUS_QUERY_TIMEOUT) as superwasp:
                s = superwasp.last_measurement()
                if s is not None:
                    h.append(fits.Card('SWSWVER', s['software_version'],
                                       'superwasp monitor server software version'), end=True)
                    h.append(fits.Card('SKYTEMP', s['sky_temp'],
                                       '[deg c] sky temperature'), end=True)
                    h.append(fits.Card('DEWPDELT', s['dew_point_delta'],
                                       '[deg c] temperature above dew point'), end=True)
        except Exception as e:
            print('failed to add superwasp headers with error: ' + str(e))
            observatory.log.error('pipelined', 'Failed to query superwasp metadata (' \
                                  + str(e) + ')')

        return h

    def __add_pipeline_header(self, camera_id, frame):
        """Adds pipeline configuration to the frame header"""
        desc = fits.Card('COMMENT', ' ---               DATA PIPELINE               --- ')
        frame.header.append(fits.Card(None, None), end=True)
        frame.header.append(desc, end=True)

        filename = ''
        if self._output_save_to_disk[camera_id]:
            ext = '.fits.gz' if self._compression_enabled else '.fits'
            filename = self._output_frame_prefix + '-' + camera_id + \
                '-{:04d}'.format(self._output_frame_number[camera_id]) + ext

        try:
            frame.header.append(fits.Card('DPSWVER', SOFTWARE_VERSION,
                                          'data pipeline software version'), end=True)
            frame.header.append(fits.Card('FILESAVE', self._output_save_to_disk[camera_id],
                                          'image has been archived to disk'), end=True)
            frame.header.append(fits.Card('FILENAME', filename, 'archived image name'), end=True)
            frame.header.append(fits.Card('IMAGETYP', self._frame_type, 'frame type'), end=True)

            if self._frame_type == 'SCIENCE':
                frame.header.append(fits.Card('OBJECT', self._frame_object,
                                              'science target name'), end=True)

        except Exception as e:
            print('failed to add pipeline header with error: ' + str(e))
            return []

    def __add_wcs_header(self, frame, camera_id, objects):
        """Solves frame WCS and adds the appropriate header keys to the frame.
           Returns a list of xpa messages that can be given to __update_previews"""
        if objects is None or len(objects) <= 0:
            return []

        width = frame.header['NAXIS1']
        height = frame.header['NAXIS2']

        try:
            table_path = '/var/tmp/' + camera_id + '-scratch.xyls'
            try:
                os.remove(table_path)
            except OSError:
                pass

            astropy.table.Table(objects).write(table_path, format='fits')

            wcs_path = '/var/tmp/' + camera_id + '-scratch.wcs'
            try:
                os.remove(wcs_path)
            except OSError:
                pass

            args = [
                '/usr/local/astrometry/bin/solve-field',
                '--overwrite', '--no-plots',
                '--axy', 'none', '--rdls', 'none', '--match', 'none',
                '--corr', 'none', '--solved', 'none',
                '--scale-units', 'arcminwidth', '--scale-high', str(WCS_SCALE_HIGH),
                '--width', str(width), '--height', str(height),
                '--ra', frame.header['TELRA'], '--dec', frame.header['TELDEC'],
                '--radius', str(WCS_SEARCH_RADIUS), table_path]

            subprocess.check_call(args, timeout=WCS_LIMIT,
                                  stdout=subprocess.DEVNULL,
                                  stderr=subprocess.DEVNULL)

            wcs_data = ''
            wcs_ignore_cards = ['SIMPLE', 'BITPIX', 'NAXIS', 'EXTEND', 'DATE', 'IMAGEW', 'IMAGEH']
            comment = 'astrometry.net wcs solution'
            with open(wcs_path) as wcs_file:
                header = wcs_file.read()
                # ds9 will only accept newline-delimited keys
                # so we need to reformat the 80-character cards
                for line in [header[i:i+80] for i in range(0, len(header), 80)]:
                    key = line[0:8].strip()
                    if '=' in line and key not in wcs_ignore_cards:
                        card = fits.Card.fromstring(line)
                        value = line[9:].split('/')[0].strip()
                        wcs_data += key + ' = ' + value + '\n'

                        frame.header.append(fits.Card(card.keyword, card.value, comment), end=True)
            return [
                ('wcs replace', wcs_data),
                ('regions', ANNOTATION_LABEL.format(width / 5, -ANNOTATION_LABEL_MARGIN,
                                                    'WCS: solved'))
            ]
        except Exception as e:
            print('failed to update wcs with error: ' + str(e))
            return [('regions', ANNOTATION_LABEL.format(width / 4, -ANNOTATION_LABEL_MARGIN,
                                                        'WCS: failed'))]

    def __detect_objects(self, frame):
        """Subtracts the frame background then returns a numpy array of (x, y, flux, fwhm) for each
           object detected by sep"""
        try:
            if frame.header['SHUTTER'] == 'CLOSED':
                return numpy.zeros((0, 4))

            platescale = frame.header['CCD-XBIN'] * OBJECT_PLATESCALE

            # TODO: use Donuts
            image = frame.data.astype(float)
            bkg = sep.Background(image)
            subtracted = image - bkg

            thresh = 5 * bkg.globalrms
            raw_objects = sep.extract(subtracted, thresh)
            objects = []
            for star in raw_objects:
                # Discard spuriously small sources
                if star['npix'] < OBJECT_MINPIX:
                    continue

                x = star['x']
                y = star['y']
                a = star['a']
                b = star['b']
                theta = star['theta']
                kronrad, flag = sep.kron_radius(subtracted, x, y, a, b, theta, 6.0)
                if flag != 0:
                    continue

                flux, _, flag = sep.sum_ellipse(subtracted, x, y, a, b, theta, 2.5 * kronrad,
                                                subpix=0)
                if flag != 0:
                    continue

                r, flag = sep.flux_radius(subtracted, x, y, 6.0 * a, 0.5, normflux=flux, subpix=5)
                if flag != 0:
                    continue

                objects.append((x, y, flux, 2 * r * platescale))

            dtype = [('X', float), ('Y', float), ('FLUX', float), ('FWHM', float)]
            return numpy.array(objects, dtype=dtype)
        except Exception as e:
            print('failed to extract objects with error: ' + str(e))
            return numpy.zeros((0, 4))

    def __add_fwhm_header(self, frame, objects):
        """Adds the median fwhm to the frame header"""
        try:
            if objects is None or len(objects) <= 0:
                return []

            median = round(numpy.median(objects['FWHM']), 2)
            frame.header.append(fits.Card('MEDFWHM', median, '[arcsec] median estimated FWHM'),
                                end=True)
            frame.header.append(fits.Card('FWHMCNT', len(objects),
                                          'number of sources used to estimate FWHM'), end=True)
        except Exception as e:
            print('failed to calculate fwhm with error: ' + str(e))

    def __add_intensity_stats_header(self, frame):
        """Estimates and the median PSF FWHM and returns a list of (x,y,r) tuples that can be
           used for the live preview overlays"""

        try:
            frame.header.append(fits.Card('MEANCNTS', round(numpy.mean(frame.data), 2),
                                          'mean frame counts'), end=True)
            frame.header.append(fits.Card('MEDCNTS', round(numpy.median(frame.data), 2),
                                          'median frame counts'), end=True)
            frame.header.append(fits.Card('STDCNTS', round(numpy.std(frame.data), 2),
                                          'standard deviation of frame counts'), end=True)
        except Exception as e:
            print('failed to calculate intensity stats with error: ' + str(e))
            return []

    def __update_previews(self, camera_id, hdulist=None, xpa_messages=None,
                          unregister_on_error=True):
        """Update registered preview clients with new data
           if hdulist is non-None the frame will be updated
           any xpa_messages will then be sent vertabim"""

        for address in self._ds9_preview_addresses[camera_id][:]:
            try:
                p = pyds9.DS9(address, start=False, wait=1)
                if hdulist is not None:
                    p.set_pyfits(hdulist)
                if xpa_messages is not None:
                    for message in xpa_messages:
                        if isinstance(message, tuple):
                            p.set(message[0], message[1])
                        else:
                            p.set(message)
            except Exception as e:
                if unregister_on_error:
                    self._ds9_preview_addresses[camera_id].remove(address)
                    print('save thread: unregistering preview {}: {}'.format(address, e))

    def __generate_header_annotations(self, camera_id, frame, objects):
        """Returns the XPA commands to display the time and exposure on the live previews"""
        width = frame.header['NAXIS1']
        height = frame.header['NAXIS2']
        date = frame.header['DATE-OBS']
        exptime = frame.header['EXPTIME']
        arm = frame.header['INSTRARM']

        saved = 'SAVED' if self._output_save_to_disk[camera_id] else 'NOT SAVED'
        annotations = [
            ('regions', ANNOTATION_LABEL.format(width / 4, height + ANNOTATION_LABEL_MARGIN, date)),
            ('regions', ANNOTATION_LABEL.format(3 * width / 4, height + ANNOTATION_LABEL_MARGIN,
                                                '{} @ {:.1f}s'.format(arm, exptime))),
            ('regions', ANNOTATION_LABEL.format(width / 2, -ANNOTATION_LABEL_MARGIN, saved))
        ]

        if 'MEDFWHM' in frame.header:
            fwhm_label = ANNOTATION_LABEL.format(
                4 * width / 5, -ANNOTATION_LABEL_MARGIN,
                'FWHM: ' + str(frame.header['MEDFWHM']) + ' arcsec')
            annotations.append(('regions', fwhm_label))

        if objects is not None and len(objects) > 0:
            for o in objects:
                cr = 'image; circle({},{},{}) #select=0 color=red'.format(o[0] + 1, o[1] + 1, o[3])
                annotations.append(('regions', cr))
        return annotations

    def __update_dashboard(self, camera_id, frame):
        try:
            # Trim overscan regions
            r = re.search(r'^\[(\d+):(\d+),(\d+):(\d+)\]$', frame.header['IMAG-RGN']).groups()
            data = frame.data[int(r[2])-1:int(r[3]), int(r[0])-1:int(r[1])]

            fwhm = -1 if 'MEDFWHM' not in frame.header else frame.header['MEDFWHM']
            metadata = {
                'date': frame.header['DATE-OBS'],
                'exptime': frame.header['EXPTIME'],
                'saved': frame.header['FILESAVE'],
                'filename': frame.header['FILENAME'],
                'fwhm': fwhm
            }

            scaled = rescale_image_data(data, self._dashboard_min_threshold,
                                        self._dashboard_max_threshold)
            png = Image.fromarray(scaled).convert('RGB')

            # Red arm flips vertically only
            # Blue arm flips horizontally and vertically
            png = ImageOps.flip(png)
            if camera_id == 'BLUE':
                png = ImageOps.mirror(png)

            png.save('/var/tmp/dashboard-' + camera_id + '.png', 'PNG', clobber=True)
            png.thumbnail((self._dashboard_thumb_size, self._dashboard_thumb_size))
            png.save('/var/tmp/dashboard-' + camera_id + '-thumb.png', 'PNG', clobber=True)
            with open('/var/tmp/dashboard-' + camera_id + '.json', 'w') as outfile:
                json.dump(metadata, outfile)
        except Exception as e:
            print('failed to generate dashboard preview with error: ' + str(e))
            observatory.log.error('pipelined', 'Failed to generate dashboard data (' \
                                  + str(e) + ')')

    def __process_frames(self, camera_id):
        process_queue = self._process_queue[camera_id]
        while True:
            # Blocks until a frame is available for processing
            loadpath = process_queue.get()

            try:
                start = datetime.datetime.utcnow()
                with fits.open(loadpath) as hdulist:
                    frame = hdulist[0]
                    print('loaded frame ' + loadpath)

                    steps = ['headers']
                    self.__add_telescope_header(frame)
                    self.__add_environment_header(frame)
                    self.__add_pipeline_header(camera_id, frame)

                    if self._intensity_stats_enabled:
                        self.__add_intensity_stats_header(frame)
                        steps.append('intstats')

                    objects = None
                    if self._fwhm_enabled or self._wcs_enabled:
                        objects = self.__detect_objects(frame)
                        steps.append('objdetect')

                    if self._fwhm_enabled:
                        self.__add_fwhm_header(frame, objects)
                        steps.append('fwhm')

                    # Update previews before the relatively-slow WCS step
                    annotations = self.__generate_header_annotations(camera_id, frame, objects)
                    self.__update_previews(camera_id, hdulist, annotations)

                    if self._wcs_enabled:
                        wcs = self.__add_wcs_header(frame, camera_id, objects)
                        self.__update_previews(camera_id, xpa_messages=wcs)
                        steps.append('wcs')

                    if self._output_save_to_disk[camera_id]:
                        ext = '.fits.gz' if self._compression_enabled else '.fits'
                        filename = self._output_frame_prefix + '-' + camera_id + \
                            '-{:04d}'.format(self._output_frame_number[camera_id]) + ext
                        savepath = str(self._output_directory / filename)

                        tmp = '.tmp.fits.gz' if self._compression_enabled else '.tmp.fits'
                        tmpname = self._output_frame_prefix + '-' + camera_id + \
                            '-{:04d}'.format(self._output_frame_number[camera_id]) + tmp

                        tmppath = str(self._output_directory / tmpname)

                        # Simulate an atomic write by writing to a temporary file then renaming
                        frame.writeto(tmppath, clobber=True)
                        shutil.move(tmppath, savepath)

                        print('Saved frame ' + filename)
                        observatory.log.info('pipelined', 'Saved frame ' + filename)
                        self._output_frame_number[camera_id] += 1
                        steps.append('compress' if self._compression_enabled else 'save')

                    if self._dashboard_enabled:
                        self.__update_dashboard(camera_id, frame)
                        steps.append('dashboard')

                process_time = round((datetime.datetime.utcnow() - start).total_seconds(), 1)

                print('processed ' + loadpath + ' in ' + str(process_time) + ' seconds')
                observatory.log.info('pipelined', 'Processed ' + loadpath + ' in ' \
                                     + str(round(process_time, 2)) + 's (' + ', '.join(steps) + ')')

            except Exception as e:
                print('Unexpected exception while processing ' + loadpath + ': ' + str(e))
            finally:
                try:
                    os.remove(loadpath)
                    print('Deleting temporary frame: ' + loadpath)
                except Exception:
                    print('Failed to delete temporary frame: ' + loadpath)
                process_queue.task_done()

    @Pyro4.expose
    def notify_frame(self, camera_id, path):
        """Called by the camera daemons to notify that a new frame has been saved to disk"""

        # Guide offsets should be applied ASAP
        if self._guide_camera_id == camera_id:
            # The guiding only cares about the latest frame
            with self._guide_condition:
                self._guide_latest_frame = path
                self._guide_condition.notify()
            print('updated guide frame', camera_id, path)

        if camera_id not in self._process_queue:
            raise Exception('Unknown camera id ' + camera_id)

        # Block until the processing thread has completed the previous frame.
        # We require a direct hand-off from the camera to the pipeline to prevent accidental
        # backlogs which would desync the real-time processing done by the processing thread.
        process_queue = self._process_queue[camera_id]
        wait_start = datetime.datetime.utcnow()
        process_queue.join()
        process_queue.put(path)
        wait = (datetime.datetime.utcnow() - wait_start).total_seconds()

        if wait > 0.1:
            print('WARNING: ' + camera_id + ' camera blocked for ' + str(wait) + 's by pipeline')
            observatory.log.warning('pipelined', camera_id + ' camera blocked for ' + str(wait) \
                                    + 's by pipeline')

    @Pyro4.expose
    def set_guide_camera(self, camera_id, background_tile_size=0):
        """Sets the camera ID to use for guiding"""
        with TryLock(self._command_lock) as success:
            if not success:
                return CommandStatus.Blocked

            self._guide_camera_id = camera_id
            self._guide_background_tile_size = background_tile_size
            self._guide_reference_frame = None
            self._guide_last_guide_frame = None
            observatory.log.info('pipelined', 'Configured autoguiding with ' + camera_id \
                                 + ' camera and ' + str(background_tile_size) + ' px tiles')
            return CommandStatus.Succeeded

    @Pyro4.expose
    def set_archive_directory(self, directory):
        """Sets the output frame directory"""
        with TryLock(self._command_lock) as success:
            if not success:
                return CommandStatus.Blocked

            # Check that the directory exists and is writable
            try:
                path = pathlib.Path(directory).resolve()
                testfile = tempfile.TemporaryFile(dir=str(path))
                self._output_directory = path
                testfile.close()
            except Exception:
                return CommandStatus.DirectoryNotWritable

            observatory.log.info('pipelined', 'Frame archive directory set to ' + str(path))
            return CommandStatus.Succeeded

    @Pyro4.expose
    def set_output_frame_prefix(self, prefix):
        """Sets the output frame prefix"""
        with TryLock(self._command_lock) as success:
            if not success:
                return CommandStatus.Blocked

            self._output_frame_prefix = prefix

            observatory.log.info('pipelined', 'Frame prefix set to ' + prefix)
            return CommandStatus.Succeeded

    @Pyro4.expose
    def set_archive(self, camera_id, enabled):
        """Enable or disable archiving to disk"""
        with TryLock(self._command_lock) as success:
            if not success:
                return CommandStatus.Blocked

            self._output_save_to_disk[camera_id] = enabled
            observatory.log.info('pipelined', 'Frame archiving for ' + camera_id \
                                 + (' enabled' if enabled else ' disabled'))
            return CommandStatus.Succeeded

    @Pyro4.expose
    def register_preview(self, camera_id, address):
        """Register a ds9 window for previews"""
        with TryLock(self._command_lock) as success:
            if not success:
                return CommandStatus.Blocked

            self._ds9_preview_addresses[camera_id].append(address)

            return CommandStatus.Succeeded

    @Pyro4.expose
    def set_output_frame_number(self, camera_id, number):
        """Sets the output frame number"""
        with TryLock(self._command_lock) as success:
            if not success:
                return CommandStatus.Blocked

            self._output_frame_number[camera_id] = int(number)

            observatory.log.info('pipelined', 'Frame number for ' + camera_id + ' set to ' \
                                 + str(number))
            return CommandStatus.Succeeded

    @Pyro4.expose
    def set_frame_object(self, object_name):
        """Sets the output frame number"""
        with TryLock(self._command_lock) as success:
            if not success:
                return CommandStatus.Blocked

            self._frame_object = object_name

            observatory.log.info('pipelined', 'Frame object set to ' + object_name)
            return CommandStatus.Succeeded

    @Pyro4.expose
    def set_frame_type(self, frame_type):
        """Sets the output frame number"""
        with TryLock(self._command_lock) as success:
            if not success:
                return CommandStatus.Blocked

            self._frame_type = frame_type

            observatory.log.info('pipelined', 'Frame type set to ' + frame_type)
            return CommandStatus.Succeeded


    @Pyro4.expose
    def set_dashboard_min_threshold(self, percent):
        """Sets the minimum contrast percentage for the web dashboard preview"""
        self._dashboard_min_threshold = percent
        observatory.log.info('pipelined', 'Dashboard preview min threshold set to ' \
                             + str(percent) + '%')
        return CommandStatus.Succeeded

    @Pyro4.expose
    def set_dashboard_max_threshold(self, percent):
        """Sets the maximum contrast percentage for the web dashboard preview"""
        self._dashboard_max_threshold = percent
        observatory.log.info('pipelined', 'Dashboard preview min threshold set to ' \
                             + str(percent) + '%')
        return CommandStatus.Succeeded

    @Pyro4.expose
    def set_dashboard_thumb_size(self, max_size):
        """Sets the maximum dimension of the web dashboard preview thumbnails"""
        self._dashboard_thumb_size = max_size
        observatory.log.info('pipelined', 'Dashboard preview thumbnail size set to ' \
                             + str(max_size) + ' px')
        return CommandStatus.Succeeded

    @Pyro4.expose
    def set_wcs(self, enabled):
        """Enable or disable wcs solutions"""
        with TryLock(self._command_lock) as success:
            if not success:
                return CommandStatus.Blocked

            self._wcs_enabled = enabled
            observatory.log.info('pipelined', 'WCS solution ' \
                                 + ('enabled' if enabled else 'disabled'))
            return CommandStatus.Succeeded

    @Pyro4.expose
    def set_fwhm(self, enabled):
        """Enable or disable fwhm calculations"""
        with TryLock(self._command_lock) as success:
            if not success:
                return CommandStatus.Blocked

            self._fwhm_enabled = enabled
            observatory.log.info('pipelined', 'FWHM calculation ' \
                                 + ('enabled' if enabled else 'disabled'))
            return CommandStatus.Succeeded

    @Pyro4.expose
    def set_intensity_stats(self, enabled):
        """Enable or disable intensity statistics solutions"""
        with TryLock(self._command_lock) as success:
            if not success:
                return CommandStatus.Blocked

            self._intensity_stats_enabled = enabled
            observatory.log.info('pipelined', 'Intensity statistics ' \
                                 + ('enabled' if enabled else 'disabled'))
            return CommandStatus.Succeeded

    @Pyro4.expose
    def set_dashboard(self, enabled):
        """Enable or disable dashboard updates"""
        with TryLock(self._command_lock) as success:
            if not success:
                return CommandStatus.Blocked

            self._dashboard_enabled = enabled
            observatory.log.info('pipelined', 'Dashboard updates ' \
                                 + ('enabled' if enabled else 'disabled'))
            return CommandStatus.Succeeded

    @Pyro4.expose
    def set_compression(self, enabled):
        """Enable or disable gz compression"""
        with TryLock(self._command_lock) as success:
            if not success:
                return CommandStatus.Blocked

            self._compression_enabled = enabled
            observatory.log.info('pipelined', 'Frame compression ' \
                                 + ('enabled' if enabled else 'disabled'))
            return CommandStatus.Succeeded

    @Pyro4.expose
    def report_status(self):
        """Returns a dictionary containing the current pipeline state"""
        next_filename = {}
        ext = '.fits.gz' if self._compression_enabled else '.fits'

        for cam in CAMERAS:
            next_filename[cam] = self._output_frame_prefix + '-' + cam \
                + '-{:04d}'.format(self._output_frame_number[cam]) + ext

        return {
            'guide_camera_id': self._guide_camera_id,
            'guide_background_tile_size': self._guide_background_tile_size,
            'guide_reference_frame': self._guide_reference_frame,
            'guide_last_guide_frame': self._guide_last_guide_frame,

            'wcs_enabled': self._wcs_enabled,
            'fwhm_enabled': self._fwhm_enabled,
            'intensity_stats_enabled': self._intensity_stats_enabled,
            'dashboard_enabled': self._dashboard_enabled,
            'compression_enabled': self._compression_enabled,

            'next_filename': next_filename,
            'archive_enabled': self._output_save_to_disk,
            'archive_directory': str(self._output_directory),

            'dashboard_min_threshold': self._dashboard_min_threshold,
            'dashboard_max_threshold': self._dashboard_max_threshold,
            'dashboard_thumb_size': self._dashboard_thumb_size,

            'frame_type': self._frame_type,
            'frame_object': self._frame_object
        }

if __name__ == '__main__':
    observatory.daemons.onemetre_pipeline.launch(PipelineDaemon())
